# @package _global_

defaults:
  - /model: pretrain_2x # equivalent to flat_enc_dec without flattening
  - /dataset: base
model:
  causal: true
  subject_embed_strategy: EmbedStrat.token
  task:
    mask_ratio: 0.75 # for efficiency
  readin_strategy: EmbedStrat.unique_project
  readout_strategy: EmbedStrat.unique_project
  readin_compress: False
dataset:
  max_arrays: 1
  max_channels: 288
  datasets:
  - odoherty_rtt-Indy.*
  exclude_datasets:
  - odoherty_rtt-Indy-20160407_02 # First indy session
  - odoherty_rtt-Indy-20160627_01 # Original
  - odoherty_rtt-Indy-20161005_06
  - odoherty_rtt-Indy-20161026_03
  - odoherty_rtt-Indy-20170131_02 # Last indy sesison
train:
  patience: 250 # Extra generous patience.
  autoscale_batch_size: False
  batch_size: 256
notes: "Sorted, Stitched NDT1 (time only). ~2.5M params for encoder, 5M params for stitching IO. Autobsz ~2048."