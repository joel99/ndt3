# @package _global_
defaults:
- _default

model:
  session_embed_token_count: 1 # Keep it low for maybe easier FT?
  neurons_per_token: 32
  causal: True
  task:
    mask_ratio: 0.5 # No encoded covariates to begin with
  lr_ramp_steps: 50
  transformer:
    n_layers: 6
  use_full_encode: True
dataset:

  data_keys:
  - DataKey.spikes
  - DataKey.bhvr_vel
  - DataKey.constraint
  tokenize_covariates: True
train:
  patience: 50
  autoscale_batch_size: true
  effective_batch_size: 512
