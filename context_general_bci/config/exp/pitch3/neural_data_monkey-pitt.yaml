# @package _global_

defaults:
  - _default
model:
  lr_decay_steps: 15000
  task:
    tasks:
    - ModelTask.spike_infill
    - ModelTask.kinematic_infill
    - ModelTask.constraints
    - ModelTask.return_context
    task_weights: [1.0, 1.0, 0., 0.]

dataset:
  datasets:
  # All cropped trialized data, but ~100K trials.
  - churchland.*
  - gallego.*
  - dyer_co.*
  - miller.*
  - odoherty_rtt.*
  - delay.*
  - pitt_broad.*
train:
  # batch_size: 24 # 40G
  # batch_size: 56 # 80G
  batch_size: 32 # 40G
  # batch_size: 64 # 80G
  patience: 50
  # effective_batch_size: 2048
  effective_batch_size: 8192
# load_from_id: data_monkey-pitt-xfhobr8j # Loaded at around 2.5K steps, 40 epochs. Picked up because 6K doesn't look like enough.
# TODO experiment with pushing the LR up, courtesy of QK