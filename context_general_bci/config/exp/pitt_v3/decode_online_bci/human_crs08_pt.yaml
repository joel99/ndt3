# @package _global_
defaults:
  - _default
train:
  autoscale_batch_size: True # For some unknown reason, this is critical to perf. Model is brittle to training noise (not enough data)
  # autoscale_batch_size: false
  patience: 200
  max_batch_size: 32
dataset:
  augmentations:
  - rand_crop_time # preset to crop to 1s
  datasets:
  - observation_CRS08.*
  exclude_datasets:
  # - CRS08_13.* # Ok, there's 13Home, which is NDT, and 13Lab, which is older, observation, 6/26.
  - CRS08_14.*
  - CRS08_15.*
  - CRS08_16.*
model:
  transformer:
    n_layers: 6
  task:
    mask_ratio: 0.1
  # lr_ramp_steps: 100
inherit_exp: ""
inherit_tag: ""