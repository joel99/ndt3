# @package _global_

defaults:
  - _default
model:
  transformer:
    n_layers: 12
    n_heads: 16
  hidden_size: 2048
dataset:
  max_tokens: 4092 # For subsequent curricula
train:
  batch_size: 32
notes: "300M sparse reward"