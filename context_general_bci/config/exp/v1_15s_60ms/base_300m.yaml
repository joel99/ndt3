# @package _global_

defaults:
  - _default
model:
  transformer:
    n_layers: 12
    n_heads: 16
  hidden_size: 2048
train:
  batch_size: 16 # on 80G
notes: "300M sparse reward"